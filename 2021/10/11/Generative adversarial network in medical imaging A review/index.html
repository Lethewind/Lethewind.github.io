<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Generative adversarial network in medical imaging A review | Lethe</title><meta name="keywords" content="GAN, 生成对抗网络, Medical imaging, 医学图像"><meta name="author" content="Farewellswind"><meta name="copyright" content="Farewellswind"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Generative adversarial network in medical imaging: A reviewAbstractGenerative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data gener">
<meta property="og:type" content="article">
<meta property="og:title" content="Generative adversarial network in medical imaging A review">
<meta property="og:url" content="http://lethewind.github.io/2021/10/11/Generative%20adversarial%20network%20in%20medical%20imaging%20A%20review/index.html">
<meta property="og:site_name" content="Lethe">
<meta property="og:description" content="Generative adversarial network in medical imaging: A reviewAbstractGenerative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data gener">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg">
<meta property="article:published_time" content="2021-10-11T05:41:49.000Z">
<meta property="article:modified_time" content="2021-10-11T05:43:05.793Z">
<meta property="article:author" content="Farewellswind">
<meta property="article:tag" content="GAN, 生成对抗网络, Medical imaging, 医学图像">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg"><link rel="shortcut icon" href="https://z3.ax1x.com/2021/09/18/4lWGh6.png"><link rel="canonical" href="http://lethewind.github.io/2021/10/11/Generative%20adversarial%20network%20in%20medical%20imaging%20A%20review/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Generative adversarial network in medical imaging A review',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-10-11 13:43:05'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="Lethe" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://z3.ax1x.com/2021/09/18/4lguXd.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://z3.ax1x.com/2021/09/18/4lfS4x.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Lethe</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Generative adversarial network in medical imaging A review</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-11T05:41:49.000Z" title="发表于 2021-10-11 13:41:49">2021-10-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-11T05:43:05.793Z" title="更新于 2021-10-11 13:43:05">2021-10-11</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Generative adversarial network in medical imaging A review"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Generative-adversarial-network-in-medical-imaging-A-review"><a href="#Generative-adversarial-network-in-medical-imaging-A-review" class="headerlink" title="Generative adversarial network in medical imaging: A review"></a>Generative adversarial network in medical imaging: A review</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross- modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.</p>
<p>生成对抗网络在计算机视觉社区中获得了很多关注，因为它们无需对概率密度函数进行显式建模即可生成数据。判别器带来的对抗性损失提供了一种巧妙的方法，可以将未标记的样本纳入训练并施加更高阶的一致性。这已被证明在许多情况下都很有用，例如域适应、数据增强和图像到图像的转换。这些特性吸引了医学成像界的研究人员，我们已经看到在许多传统和新颖的应用中迅速采用，例如图像重建、分割、检测、分类和跨模态合成。 根据我们的观察，这种趋势将继续下去，因此我们对使用对抗性训练的医学成像的最新进展进行了整理，希望使对这项技术感兴趣的研究人员受益。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>With the resurgence of deep learning in computer vision starting from 2012 ( Krizhevsky et al., 2012 ), the adoption of deep learning methods in medical imaging has increased dramatically. It is estimated that there were over 400 papers published in 2016 and 2017 in major medical imaging related conference venues and journals ( Litjens et al., 2017 ). The wide adoption of deep learning in the medical imaging community is due to its demonstrated potential to complement image interpretation and augment image representation and classification. In this article, we focus on one of the most interesting recent breakthroughs in the field of deep learning - generative adversarial networks (GANs) - and their potential applications in the field of medical imaging.</p>
<p>随着 2012 年计算机视觉深度学习的复苏（Krizhevsky et al., 2012），深度学习方法在医学成像中的应用急剧增加。 据估计，2016年和2017年在主要医学影像相关会议和期刊上发表的论文超过400篇（Litjens et al., 2017）。深度学习在医学成像界的广泛采用是由于其在补充图像解释和增强图像表示和分类方面表现出的潜力。在本文中，我们关注深度学习领域最近最有趣的突破之一——生成对抗网络 (GAN)——及其在医学成像领域的潜在应用。</p>
<p>GANs are a special type of neural network model where two networks are trained simultaneously, with one focused on image generation and the other centered on discrimination. The ad-versarial training scheme has gained attention in both academia and industry due to its usefulness in counteracting domain shift, and effectiveness in generating new image samples. This model has achieved state-of-the-art performance in many image generation tasks, including text-to-image synthesis ( Xu et al., 2017 ), super-resolution ( Ledig et al., 2017 ), and image-to-image translation ( Zhu et al., 2017 ).</p>
<p>GAN 是一种特殊类型的神经网络模型，其中两个网络同时训练，一个专注于图像生成，另一个专注于判别。由于对抗域迁移的有用性和生成新图像样本的有效性，对抗性训练方案在学术界和业界都受到关注。该模型在许多图像生成任务中取得了最先进的性能，包括文本到图像合成（Xu et al., 2017）、超分辨率（Ledig et al., 2017）和图像到图像转换（Zhu et al., 2017）。</p>
<p>Unlike deep learning which has its roots traced back to the 1980s ( Fukushima and Miyake, 1982 ), the concept of ad- versarial training is relatively new with significant recent progress ( Goodfellow et al., 2014 ). This paper presents a gen- eral overview of GANs, describes their promising applications in medical imaging, and identifies some remaining challenges that need to be solved to enable their successful application in other medical imaging related tasks.</p>
<p>与起源于 1980 年代（Fukushima 和 Miyake，1982）的深度学习不同，对抗训练的概念相对较新，并且最近取得了重大进展（Goodfellow 等，2014）。 本文概述了 GAN，描述了它们在医学成像中的有前景的应用，并确定了一些需要解决的剩余挑战，以使其成功应用于其他医学成像相关任务。</p>
<p>To present a comprehensive overview of all relevant works on GANs in medical imaging, we searched databases including PubMed, arXiv, proceedings of the International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), SPIE Medical Imaging, IEEE International Symposium on Biomedical Imaging (ISBI), and International conference on Medical Imaging with Deep Learning (MIDL). We also incorporated cross referenced works not identified in the above search process. Since there are research publications coming out every month, without losing generality, we set the cut offtime of the search as January 1st, 2019. Works on arXiv that report only preliminary results are excluded from this review. Descriptive statistics of these papers based on task, imaging modality and year can be found in Fig. 1 .</p>
<p>为了全面概述医学成像中 GAN 的所有相关工作，我们检索了包括 PubMed、arXiv、医学图像计算和计算机辅助干预国际会议 (MICCAI)、SPIE 医学成像、IEEE 生物医学成像国际研讨会论文集在内的数据库 (ISBI) 和深度学习医学成像国际会议 (MIDL)。 我们还合并了上述搜索过程中未确定的交叉引用作品。 由于每个月都有研究发表，在不失一般性的情况下，我们将搜索截止时间设置为 2019 年 1 月 1 日。 arXiv 上仅报告初步结果的作品被排除在本次审查之外。 这些论文基于任务、成像方式和年份的描述性统计可以在图 1 中找到。</p>
<p><img src="https://i.loli.net/2021/10/09/zG6Bkgs3O2VWSpF.png" alt="image-20211009191958548"></p>
<p>Fig. 1. (a) Categorization of GAN related papers according to canonical tasks. (b) Categorization of GAN related papers according to imaging modality. (c) Number of GAN related papers published from 2014. Note that some works performed various tasks and conducted evaluation on datasets with different modalities. We counted these works multiple times in plotting these graphs. Works related to cross domain image transfer were counted based on the source domain. The statistics presented in figure (a) and (b) are based on papers published on or before January 1st, 2019.</p>
<p>图1..（a） 根据规范任务对与GAN相关的论文进行分类。（b） 根据影像学形态分类与GAN相关的论文。（c） 2014年发表的与GAN相关的论文数量。请注意，一些工作执行了各种任务，并对不同模式的数据集进行了评估。在绘制这些图表时，我们对这些作品进行了多次计数。基于源域统计与跨域图像传输相关的工作。图（a）和（b）中的统计数据基于2019年1月1日或之前发表的论文。</p>
<p>The remainder of the paper is structured as follows. We begin with a brief introduction of the principles of GANs and some of its structural variants in Section 2 . It is followed by a com- prehensive review of medical image analysis tasks using GANs in Section 3 including but not limited to the fields of radiology, histopathology and dermatology. We categorize all the works according to canonical tasks: reconstruction, image synthesis, segmentation, classification, detection, registration, and others. Section 4 summarizes the review and discusses prospective applications and identifies open challenges.</p>
<p>本文其余部分的结构如下。在第2节中，我们首先简要介绍GANs的原理及其一些结构变体。第3节对使用GANs的医学图像分析任务进行了全面回顾，包括但不限于放射学、组织病理学和皮肤学领域。我们根据标准任务对所有工作进行分类：重建、图像合成、分割、分类、检测、配准等。第4节总结了审查，讨论了潜在的应用，并确定了存在的挑战。</p>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2. Background"></a>2. Background</h2><h3 id="2-1-Vanilla-GAN"><a href="#2-1-Vanilla-GAN" class="headerlink" title="2.1 Vanilla GAN"></a>2.1 Vanilla GAN</h3><p>The vanilla GAN ( Goodfellow et al., 2014 ) is a generative model that was designed for directly drawing samples from the desired data distribution without the need to explicitly model the underlying probability density function. It consists of two neural networks: the generator $G$ and the discriminator $D$. The input to $G$, $z$ is pure random noise sampled from a prior distribution $p(z)$, which is commonly chosen to be a Gaussian or a uniform distribution for simplicity. The output of $G$, $x_g$ is expected to have visual similarity with the real sample $x_r$ that is drawn from the real data distribution $p_r(x)$. We denote the non-linear mapping function learned by $G$ parametrized by $θ_g$ as $x_g=G(z;θ_g)$ . The input to $D$ is either a real or generated sample. The output of $D$, $y_1$ is a single value indicating the probability of the input being a real or fake sample. The mapping learned by $D$ parametrized by $θ_d$ is denoted as $y_1=D(x;θ_d) $. The generated samples form a distribution $p_g ( x )$ which is desired to be an approximation of $p_r( x )$ after successful training. The top of Fig. 2 shows an illustration of a vanilla GAN’s configuration. G in this example is generating a 2D CT slice depicting a lung nodule.</p>
<p>Vanilla GAN（Goodfello et al.，2014）是一种生成模型，设计用于直接从所需数据分布中提取样本，而无需显式建模基础概率密度函数。它由两个神经网络组成：生成器$G$和判别器$D$。$G$的输入$z$是从先验分布$p(z)$中采样的纯随机噪声，为简单起见，通常选择高斯分布或均匀分布。$G$的输出$x_g$预期与从真实数据分布$p_r(x)$中提取的真实样本$x_r$具有视觉相似性。我们将$G$学习的非线性映射函数表示为$x_g=G(z;θ_G)$。$D$的输入要么是真实的样本，要么是生成的样本。$D$的输出$y_1$是一个单一值，表示输入为真实或虚假样本的概率。由$D$学习并由$θ_d$参数化的映射表示为$y_1=D(x;θ_D)$。生成的样本形成一个分布$p_g(x)$，在成功训练后，该分布期望是$p_r(x)$的近似值。图2的顶部示出了GAN的配置的图示。在本例中，G生成了一个描绘肺结节的2D CT切片。</p>
<p><img src="https://i.loli.net/2021/10/09/A5HREaYkfqcQoUF.png" alt="image-20211009201708590"></p>
<p>Fig. 2. Schematic view of the vanilla GAN for synthesis of lung nodule on CT images. Top of the figure shows the network configuration. The part below shows the input, output and the internal feature representations of the generator $G$ and discriminator $D$. $G$ transforms a sample $z$ from $p ( z )$ into a generated nodule $x_g$ . $D$ is a binary classifier that differentiates the generated and real images of lung nodule formed by $x_g$ and $x_r$ respectively.</p>
<p>图2.CT图像上用于合成肺结节的香草甘的示意图。图的顶部显示了网络配置。下面的部分显示了生成器$G$和判别器$D$的输入、输出和内部特征表示。$G$将样本$z$从$p(z)$转换为生成的结节$x_g$，$D$是一个二值分类器，用于区分由$x_g$和$x_r$分别形成的肺结节的生成图像和真实图像。</p>
<p>$D$’s objective is to differentiate these two groups of images whereas the generator $G$ is trained to confuse the discriminator $D$ as much as possible. Intuitively, $G$ could be viewed as a forger trying to produce some quality counterfeit material, and $D$ could be regarded as the police officer trying to detect the forged items. In an alternative view, we can perceive $G$ as receiving a reward signal from $D$ depending upon whether the generated data is accurate or not. The gradient information is back propagated from $D$ to $G$, so $G$ adapts its parameters in order to produce an output image that can fool $D$. The training objectives of $D$ and $G$ can be expressed mathematically as:</p>
<p>$D$的目标是区分这两组图像，而生成器$G$经过训练以尽可能混淆判别器$D$。直觉上，$G$可以被视为一个试图制造一些高质量假冒材料的伪造者，$D$可以被视为试图检测伪造物品的警官。在另一种观点中，我们可以将$G$视为从$D$接收到激励信号，这取决于生成的数据是否准确。梯度信息从$D$反向传播到$G$，因此$G$调整其参数，以产生可以愚弄$D$的输出图像。$D$和$G$的训练目标可以数学表示为：</p>
<p><img src="https://i.loli.net/2021/10/09/5WkXtMze3TYsUl4.png" alt="image-20211009203028462"></p>
<p>As can be seen, $D$ is simply a binary classifier with a maximum log likelihood objective. If the discriminator $D$ is trained to optimality before the next generator $G$ updates, then minimizing $\cal{L}\rm{^{GAN}_G}$ is proven to be equivalent to minimizing the Jensen–Shannon (JS) divergence between $p_r(x)$ and $p_g(x)$ ( Goodfellow et al., 2014 ). The desired outcome after training is that samples formed by $x_g$ should approximate the real data distribution $p_r(x)$.</p>
<p>可以看出，$D$只是一个具有<a href="####%E6%9C%80%E5%A4%A7%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E7%9B%AE%E6%A0%87">最大对数似然目标</a>的二分类器。如果在下一个生成器$G$更新之前，将判别器$D$训练为最优，那么最小化$\cal{L}\rm{^{GAN}_G}$被证明等同于最小化$p_r(x)$和$p_g(x)$之间的Jensen–Shannon(JS)散度（Goodfello等人，2014）。训练后的预期结果是，由$x_g$形成的样本应近似于实际数据分布$p_r(x)$。</p>
<h3 id="2-2-Challenges-in-optimizing-GANs"><a href="#2-2-Challenges-in-optimizing-GANs" class="headerlink" title="2.2 Challenges in optimizing GANs"></a>2.2 Challenges in optimizing GANs</h3><p>The above GAN training objective is regarded as a saddle point optimization problem ( Yadav et al., 2018 ) and the training is often accomplished by gradient-based methods. G and D are trained alternately from scratch so that they may evolve together. However, there is no guarantee of balance between the training of G and D with the JS divergence. As a consequence, one network may inevitably be more powerful than the other, which in most cases is D. When D becomes too strong as opposed to G, the generated samples become too easy to be separated from real ones, thus reaching a stage where gradients from D approach zero, providing no guidance for further training of G. This hap- pens more frequently when generating high resolution images due to the difficulty of generating meaningful high frequency details.</p>
<p>上述GAN训练目标被视为<a href="####%E9%9E%8D%E7%82%B9%E4%BC%98%E5%8C%96">鞍点优化</a>问题（Yadav等人，2018年），训练通常通过基于梯度的方法完成。G和D从零开始交替训练，以便它们可以一起进化。然而，不能保证G和D的训练与<a href="####JS%E6%95%A3%E5%BA%A6">JS散度</a>之间的平衡。因此，一个网络可能不可避免地比另一个网络更强大，在大多数情况下是D。当D相对于G变得太强时，生成的样本变得太容易与真实样本分离，从而达到D的梯度接近零的阶段，没有为G的进一步训练提供指导。由于难以生成有意义的高频细节，因此在生成高分辨率图像时，此种现象会出现地更加频繁。</p>
<p>Another problem commonly faced in training GANs is mode collapse, which, as the name indicates, is a case when the distribution $p_g(x)$ learned by G focuses on a few limited modes of the data distribution $p_r(x)$. Hence instead of producing diverse images, it generates a limited set of samples.</p>
<p>在训练GANs中通常面临的另一个问题是<a href="####%E6%A8%A1%E5%BC%8F%E5%B4%A9%E6%BA%83">模式崩溃</a>，顾名思义，这是G学习的分布$p_g(x)$关注数据分布$p_r(x)$的几个有限模式的情况。因此，它不是生成不同的图像，而是生成一组有限的样本。</p>
<h3 id="2-3-Variants-of-GANs"><a href="#2-3-Variants-of-GANs" class="headerlink" title="2.3 Variants of GANs"></a>2.3 Variants of GANs</h3><h4 id="2-3-1-Varying-objective-of-D"><a href="#2-3-1-Varying-objective-of-D" class="headerlink" title="2.3.1 Varying objective of D"></a>2.3.1 Varying objective of D</h4><p>In order to stabilize training and also to avoid mode collapse, different losses for D have been proposed, such as <strong>f-divergence</strong> (f-GAN) ( Nowozin et al., 2016 ), <strong>least-square</strong> (LSGAN) ( Mao et al., 2017 ), <strong>hinge loss</strong> ( Miyato et al., 2018 ), and <strong>Wasserstein distance</strong> (WGAN, WGAN-GP) ( Arjovsky et al., 2017; Gulrajani et al., 2017 ). Among these, Wasserstein distance is arguably the most popular metric. As an alternative to the real/fake discrimination scheme, Springenberg (2015) proposed an entropy based objective where real data is encouraged to make confident class predictions (CatGAN, Fig. 3 b). In EBGAN ( Zhao et al., 2016 ) and BE- GAN ( Berthelot et al., 2017 ) ( Fig. 3 c), the commonly used encoder architecture for discriminator is replaced with an autoencoder architecture. D’s objective then becomes matching autoencoder loss distribution rather than data distribution.</p>
<p>为了稳定训练并避免模式崩溃，提出了D的不同损失，如<a href="####f%E6%95%A3%E5%BA%A6">f散度</a>（f-GAN）（Nowozin等人，2016年）、最小二乘（LSGAN）（Mao等人，2017年）、<a href="####%E9%93%B0%E9%93%BE%E6%8D%9F%E5%A4%B1">铰链损失</a>（Miyato等人，2018年）和<a href="####Wasserstein%E8%B7%9D%E7%A6%BB">Wasserstein距离</a>（WGAN，WGAN-GP）（Arjovsky等人，2017年；Gularjani等人，2017年）。其中，Wasserstein距离可以说是最流行的度量。作为真/假鉴别方案的替代方案，Springenberg（2015）提出了一种基于熵的目标，其中鼓励真实数据进行置信的类别预测（CatGAN，图3 b）。在EBGAN（Zhao等人，2016年）和BE-GAN（Berthelot等人，2017年）（图3 c）中，用于判别器的常用编码器架构被<a href="####%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84">自动编码器架构</a>取代。D的目标是匹配自动编码器的损失分布，而不是数据分布。</p>
<p>GANs themselves lack the mechanism of inferencing the underlying latent vector that is likely to encode the input. Therefore, in ALI ( Dumoulin et al., 2016 ) and BiGAN ( Donahue et al., 2016 ) ( Fig. 3 d), a separate encoder network is incorporated. D’s objective then becomes separating joint samples $(x_g,z_g)$ and $(x_r,z_r)$. In InfoGAN ( Fig. 3 e), the discriminator outputs the latent vector that encodes part of the semantic features of the generated image. The discriminator maximizes the mutual information between the generated image and the latent attribute vector the generated image is conditioned upon. After successful training, InfoGAN can explore inherent data attributes and perform conditional data generation based on these attributes. The use of class labels has been shown to further improve generated image’s quality and this information can be easily incorporated into D by enforcing D to provide class probabilities and use cross entropy loss for optimization such as used in ACGAN ( Odena et al., 2017 ) ( Fig. 3 f).</p>
<p>GANs本身缺乏推断潜在向量的机制，该潜在向量可能对输入进行编码。因此，在ALI（Dumoulin等人，2016）和BiGAN（Donahue等人，2016）（图3 d）中，合并了一个单独的编码器网络。D的目标是分离关节样本$(x_g,z_g)$和$(x_r,z_r)$。在InfoGAN（图3e）中，判别器输出对所生成图像的部分语义特征进行编码的潜在向量。判别器最大化生成图像和生成图像所基于的潜在属性向量之间的互信息。成功训练后，InfoGAN可以探索固有的数据属性，并基于这些属性执行条件数据生成。类标签的使用已被证明可进一步提高生成图像的质量，通过强制D提供类概率并使用交叉熵损失进行优化（如在ACGAN中使用的），该信息可容易地并入D中（Odena et al.，2017）（图3 f）。</p>
<h4 id="2-3-2-Varying-object-of-G"><a href="#2-3-2-Varying-object-of-G" class="headerlink" title="2.3.2 Varying object of G"></a>2.3.2 Varying object of G</h4><p>In the vanilla GAN, G transforms noise z to sample $x_g=G(z)$ . This is usually accomplished by using a decoder network to progressively increase the spatial size of the output until the desired resolution is achieved as shown in Fig. 2 . Larsen et al. (2015) proposed a variational autoencoder network (VAE) as the underlying architecture of G (VAEGAN, Fig. 3 g), where it can use pixel-wise reconstruction loss to enforce the decoder part of VAE to generate structures to match the real images.</p>
<p>在标准GAN中，$G$将噪声$z$转换为样本$x_g=G(z)$。这通常通过使用解码器网络来逐步增加输出的空间大小来实现，直到如图2所示实现所需的分辨率。Larsen et al.（2015）提出了一种变分自动编码器网络（VAE）作为G的底层架构（VAEGAN，图3 G），其中它可以使用像素级重建损失来强制VAE的解码器部分生成与真实图像匹配的结构。</p>
<p>The original setup of a GAN does not have any restrictions on the modes of data it can generate. However, if auxiliary information were provided during the generation, the GAN can be driven to output images with desired properties. A GAN in this scenario is usually referred as a conditional GAN (cGAN) and the generation process can be expressed as $x_g=G(z,c)$ .</p>
<p>GAN的原始设置对其可以生成的数据模式没有任何限制。然而，如果在生成期间提供辅助信息，则可以驱动GAN以输出具有期望特性的图像。在这种情况下，GAN通常被称为条件GAN（cGAN），并且生成过程可以表示为$x_g=G(z,c)$。</p>
<p>One of the most common conditional inputs $c$ is an image. pix2pix, the first general purpose GAN based image-to-image translation framework was proposed by Isola et al. (2016) ( Fig. 4 a). Further, task related supervision was introduced to the generator. For example, reconstruction loss for image restoration and Dice loss ( Milletari et al., 2016 ) for segmentation. This form of supervision requires aligned training pairs. Zhu et al. (2017) and Kim et al. (2017) relaxed this constraint by stitching two generators together head to toe so that images can be translated between two sets of unpaired samples ( Fig. 4 b). For the sake of simplicity, we chose CycleGAN to represent this idea in the rest of this paper. Another model named UNIT ( Fig. 4 c) can also perform unpaired image-to-image transform by combining two VAEGANs together with each one responsible for one modality but sharing the same latent space ( Liu et al., 2017a ). These image-to-image translation frameworks are very popular in the medical imaging community due to their general applicability.</p>
<p>最常见的条件输入$c$之一是图像。pix2pix是第一个基于GAN的通用图像到图像转换框架，由Isola等人（2016）提出（图4a）。此外，还向生成器引入了与任务相关的监督。例如，用于图像恢复的重建损失和用于分割的骰子损失（Milleri等人，2016）。这种形式的监督需要成对的训练。Zhu et al.（2017）和Kim et al.（2017）通过将两个生成器从头到尾缝合在一起，从而放松了这一约束，从而可以在两组未配对样本之间转换图像（图4b）。为了简单起见，我们选择了CycleGAN在本文的其余部分中表示这个想法。另一个名为UNIT的模型（图4c）也可以通过将两个Vaegan组合在一起，每个Vaegan负责一个模态，但共享相同的潜在空间来执行未配对的图像到图像变换（Liu等人，2017a）。这些图像到图像的转换框架由于其普遍适用性而在医学成像界非常流行。</p>
<p>Other than image, the conditional input can be class labels (CGAN, Fig. 3 h) ( Mirza and Osindero, 2014 ), text descriptions ( Zhang et al., 2017a ), object locations ( Reed et al., 2016a; 2016b ), surrounding image context ( Pathak et al., 2016 ), or sketches ( Sangkloy et al., 2017 ). Note that ACGAN mentioned in the previous section also has a class conditional generator.</p>
<p>除了图像，条件输入可以是类别标签（CGAN，图3h）（Mirza和Osindero，2014）、文本描述（Zhang等人，2017a）、对象位置（Reed等人，2016a；2016b）、周围图像上下文（Pathak等人，2016）或草图（Sangkloy等人，2017）。请注意，上一节中提到的ACGAN还有一个类条件生成器。</p>
<h4 id="2-3-3-Varying-architecture"><a href="#2-3-3-Varying-architecture" class="headerlink" title="2.3.3 Varying architecture"></a>2.3.3 Varying architecture</h4><p>Fully connected layers were used as the building block in vanilla GAN but later on, were replaced by fully convolutional downsampling/upsampling layers in DCGAN ( Radford et al., 2015 ).<br>DCGAN demonstrated better training stability hence quickly populated the literature. As shown in Fig. 2 , the generator in DCGAN architecture works on random input noise vector by successive upsampling operations eventually generating an image from it. Two of its important ingredients are BatchNorm ( Ioffe and Szegedy, 2015 ) for regulating the extracted feature scale, and LeakyRelu ( Maas et al., 2013 ) for preventing dead gradients. Very recently, Miyato et al. (2018) proposed a spectral normalization layer that normalized weights in the discriminator to regulate the scale of feature response values. With the training stability improved, some works have also incorporated residual connections into both the generator and discriminator and experimented with much deeper networks ( Gulrajani et al., 2017; Miyato et al., 2018 ).<br>The work in Miyato and Koyama (2018) proposed a projection based way to incorporate the conditional information instead of direct concatenation and found it to be beneficial in improving the generated image’s quality.</p>
<p>全连接层被用作vanilla GAN中的构建块，但后来被DCGAN中的<a href="####%E5%AE%8C%E5%85%A8%E5%8D%B7%E7%A7%AF%E9%87%87%E6%A0%B7">完全卷积下采样/上采样层</a>所取代（Radford et al.，2015）。<br>DCGAN表现出更好的训练稳定性，因此很快就在文献中出现。如图2所示，DCGAN结构中的生成器通过连续的上采样操作在随机输入噪声向量上工作，最终从中生成图像。它的两个重要成分是BatchNorm（Ioffe和Szegedy，2015），用于调节提取的特征尺度，以及LeakyRelu（Maas等人，2013），用于防止死梯度。最近，Miyato等人（2018）提出了一种频谱归一化层，该层对判别器中的权重进行归一化，以调节特征响应值的尺度。随着训练稳定性的提高，一些工作还将剩余连接纳入了生成器和判别器中，并对更深层次的网络进行了试验（Gullajani等人，2017年；Miyato等人，2018年）。<br>Miyato和Koyama（2018）的工作提出了一种基于投影的方法来合并条件信息，而不是直接连接，并发现它有助于提高生成图像的质量。</p>
<p>Directly generating high resolution images from a noise vector is hard, therefore some works have proposed tackling it in a progressive manner. In LAPGAN ( Fig. 3 i), Denton et al. (2015) proposed a stack of GANs, each of which adds higher frequency details into the generated image. In SGAN, a cascade of GANs is also used but each GAN generates increasingly lower level representations ( Huang et al., 2017 ), which are compared with the hierarchical representations extracted from a discriminatively trained model. Karras et al. (2017) adopted an alternate way where they progressively grow the generator and discriminator by adding new layers to them rather than stacking another GAN on top of the preceding one (PGGAN). This progressive idea was also explored in conditional setting ( Wang et al., 2018 ). More recently, Karras et al. (2019) proposed a style-based generator architecture (styleGAN) where instead of directly feeding the latent code z to the input of the generator, they transformed this code first to an intermediate latent space and then use it to scale and shift the normalized image feature responses computed from each convolution layer. Similarly, Park et al. (2019) proposed SPADE where the segmentation mask was injected to the generator via a spatially adaptive normalization layer. This conditional setup was found to better preserve the semantic layout of the mask than directly feeding the mask to the generator.</p>
<p>从噪声矢量直接生成高分辨率图像是困难的，因此一些工作建议以渐进的方式处理它。在LAPGAN（图3 i）中，Denton等人（2015）提出了一组GANs，每个GANs都向生成的图像中添加了更高频率的细节。在SGAN中，也使用了级联的GAN，但每个GAN生成越来越低级别的表示（Huang等人，2017），这与从差别训练模型中提取的层次表示进行了比较。Karras等人（2017年）采用了另一种方法，即通过向生成器和判别器添加新层，而不是在前一层（PGGAN）的顶部堆叠另一层GAN，逐步增加生成器和判别器。这种进步的想法也在条件设置中得到了探索（Wang等人，2018年）。最近，Karras et al.（2019）提出了一种基于样式的生成器体系结构（styleGAN），其中不直接将潜在代码z提供给生成器的输入，他们首先将该代码转换为一个中间潜在空间，然后使用它来缩放和移动从每个卷积层计算的归一化图像特征响应。类似地，Park等人（2019）提出了SPADE，其中<a href="####%E5%88%86%E5%89%B2%E6%8E%A9%E6%A8%A1">分割掩模</a>通过空间自适应归一化层注入生成器。发现这种条件设置比直接将掩码提供给生成器更好地保留掩码的语义布局。</p>
<p>Schematic illustrations of the most representative GANs are shown in Fig. 3 . They are GAN, CatGAN, EBGAN/BEGAN, ALI/BiGAN, InfoGAN, ACGAN, VAEGAN, CGAN, LAPGAN, SGAN. Three popular image-to-image translation cGANs (pix2pix, CycleGAN, and UNIT) are shown in Fig. 4 . For a more in-depth review and empirical evaluation of these different variants of GAN, we refer the reader to Huang et al. (2018) , Creswell et al. (2018) and Kurach et al. (2018) .</p>
<p>图3显示了最具代表性的GAN的示意图。他们是GAN、CatGAN、EBGAN/Begin、ALI/BiGAN、InfoGAN、ACGAN、VAEGAN、CGAN、LAPGAN、SGAN。图4中显示了三种流行的图像到图像转换cgan（pix2pix、CycleGAN和UNIT）。为了更深入地回顾和实证评估这些不同的GAN变体，我们请读者参考Huang et al.（2018）、Creswell et al.（2018）和Kurach et al.（2018）。</p>
<p><img src="https://i.loli.net/2021/10/10/wj32VFk1fdTqelL.png" alt="image-20211010111605807"></p>
<p>Fig. 3. A schematic view of variants of GAN. c represents the conditional vector. In CGAN and ACGAN, c is the discrete categorical code (e.g. one hot vector) that encodes class labels and in InfoGAN it can also be continuous code that encodes attributes. $x_g$ generally refers to the generated image but can also be internal representations as in<br>SGAN.</p>
<p>图3. GAN变体的示意图。c表示条件向量。在CGAN和ACGAN中，c是编码类标签的离散分类代码（例如，一个独热向量），在InfoGAN中，c也可以是编码属性的连续代码。$x_g$通常指生成的图像，但也可以是SGAN中的内部表示。</p>
<p><img src="https://i.loli.net/2021/10/10/9tgPFwyAa71hezJ.png" alt="image-20211010211408002"></p>
<p>Fig. 4. cGAN frameworks for image-to-image translation. pix2pix requires aligned training data whereas this constraint is relaxed in CycleGAN but usually suffers from performance loss. Note that in (a), we chose reconstruction loss as an example of target consistency. This supervision is task related and can take many other different forms. (c) It consists of two VAEGANs with shared latent vector in the VAE part.</p>
<p>图4. 用于图像到图像翻译的CGA框架。pix2pix需要对齐的训练数据，而这种约束在CycleGAN中是放松的，但通常会受到性能损失的影响。注意，在(a)中，我们选择重建损失作为目标一致性的示例。这种监督与任务相关，可以采取许多其他不同形式。(c)它由两个在VAE部分具有共享潜在向量的Vaegan组成。</p>
<h2 id="3-Applications-in-medical-imaging"><a href="#3-Applications-in-medical-imaging" class="headerlink" title="3. Applications in medical imaging"></a>3. Applications in medical imaging</h2><p>There are generally two ways GANs are used in medical imaging. The first is focused on the generative aspect, which can help in exploring and discovering the underlying structure of training data and learning to generate new images. This property makes GANs very promising in coping with data scarcity and patient privacy. The second focuses on the discriminative aspect, where the discriminator D can be regarded as a learned prior for normal images so that it can be used as regularizer or detector when presented with abnormal images. Fig. 5 provides examples of GAN related applications, with examples (a), (b), (c), (d), (e), (f) that focus on the generative aspect and example (g) that exploits the discriminative aspect. In the following subsections, in order to help the readers find applications of their interest, we categorized all the reviewed articles into canonical tasks: reconstruction, image synthesis, segmentation, classification, detection, registration, and others.</p>
<p>在医学成像中，GANs通常有两种使用方式。第一个重点是生成方面，这有助于探索和发现训练数据的底层结构，并学习生成新图像。这一特性使得GANs在应对数据稀缺和患者隐私方面非常有希望。第二个关注于鉴别方面，其中判别器D可以被视为正常图像的学习先验，以便当呈现异常图像时，它可以被用作正则化器或检测器。图5提供了GAN相关应用的示例，其中示例a、b、c、d、e、f侧重于生成方面，示例g利用区别方面。在下面的小节中，为了帮助读者找到他们感兴趣的应用程序，我们将所有已审阅的文章分类为规范任务：重建、图像合成、分割、分类、检测、配准等。</p>
<p><img src="https://i.loli.net/2021/10/10/Y4DblKTVL7n2Avs.png" alt="image-20211010211614963"></p>
<p>Fig. 5. Example applications using GANs. Figures are directly cropped from the corresponding papers. (a) Left side shows the noise contaminated low dose CT and right side<br>shows the denoised CT that well preserved the low contrast regions in the liver ( Yi and Babyn, 2018 ). (b) Left side shows the MR image and right side shows the synthesized<br>corresponding CT. Bone structures were well delineated in the generated CT image ( Wolterink et al., 2017a ). (c) The generated retinal fundus image have the exact vessel<br>structures as depicted in the left vessel map ( Costa et al., 2017b ). (d) Randomly generated skin lesion from random noise (a mixture of malignant and benign) ( Yi et al.,<br>2018 ). (e) An organ (lung and heart) segmentation example on adult chest X-ray. The shapes of lung and heart are regulated by the adversarial loss ( Dai et al., 2017b ). (f) The<br>third column shows the domain adapted brain lesion segmentation result on SWI sequence without training with the corresponding manual annotation ( Kamnitsas et al.,<br>2017 ). (g) Abnormality detection of optical coherence tomography images of the retina ( Schlegl et al., 2017 ).</p>
<p>图5。使用GANs的示例应用程序。数字是从相应的文件中直接裁剪出来的。(a)左侧显示噪声污染的低剂量CT，右侧显示去除噪声的CT，它很好地保留了肝脏中的低对比度区域（Yi和Babyn，2018）。(b)左侧显示MR图像，右侧显示相应的合成CT。在Wolterink等人的图像中描绘了骨骼结构。(c)生成的视网膜眼底图像具有左血管图所示的精确血管结构（Costa等人，2017b）。(d)随机噪声随机产生的皮肤损伤（恶性和良性混合）（Yi等人，2018年）。(e)成人胸部X光片上的器官（肺和心脏）分割示例。肺和心脏的形状受对抗性损失的影响（Dai等人，2017b）。(f)第三列显示了SWI序列上的域自适应脑损伤分割结果，无需使用相应的手动注释进行训练（Kamnitsas et al.，2017）。(g)视网膜光学相干断层扫描图像的异常检测（Schlegl等人，2017年）。</p>
<h3 id="3-1-Reconstruction"><a href="#3-1-Reconstruction" class="headerlink" title="3.1 Reconstruction"></a>3.1 Reconstruction</h3><p>Due to constraints in clinical settings, such as radiation dose and patient comfort, the diagnostic quality of acquired medical images may be limited by noise and artifacts. In the last decade, we have seen a paradigm shift in reconstruction methods changing from analytic to iterative and now to machine learning based methods. These data-driven learning based methods either learn to transfer raw sensory inputs directly to output images or serve as a post processing step for reducing image noise and removing artifacts. Most of the methods reviewed in this section are borrowed directly from the computer vision literature that formulate post-processing as an image-to-image translation problem where the conditioned inputs of cGANs are compromised in certain forms, such as low spatial resolution, noise contamination, under-sampling, or aliasing. One exception is for MR images where the Fourier transform is used to incorporate the raw K-space data into the reconstruction.</p>
<p>由于临床环境的限制，如辐射剂量和患者舒适度，采集的医学图像的诊断质量可能受到噪声和伪影的限制。在过去十年中，我们看到重建方法的范式转变，从分析方法转变为迭代方法，现在转变为基于机器学习的方法。这些基于数据驱动的学习方法要么学习将原始感官输入直接传输到输出图像，要么作为减少图像噪声和消除伪影的后处理步骤。本节中回顾的大多数方法都是直接从计算机视觉文献中借来的，这些文献将后处理描述为图像到图像的转换问题，其中CGAN的条件输入以某些形式受到损害，例如低空间分辨率、噪声污染、欠采样或混叠。一个例外是MR图像，其中傅里叶变换用于将原始K空间数据合并到重建中。</p>
<p>The basic pix2pix framework has been used for low dose CT denoising ( Wolterink et al., 2017b ), MR reconstruction ( Chen et al., 2018b; Kim et al., 2018; Dar et al., 2018b; Shitrit and Raviv, 2017 ), and PET denoising ( Wang et al., 2018b ). A pretrained VGGnet ( Simonyan and Zisserman, 2014 ) was further incorporated into the optimization framework to ensure perceptual similarity ( Yang et al., 2018; Yu et al., 2017; Yang et al., 2018a; Armanious et al., 2018c; Mahapatra, 2017 ). Yi and Babyn (2018) introduced a pretrained sharpness detection network to explicitly constrain the sharpness of the denoised CT especially for low contrast regions. Mahapatra (2017) computed a local saliency map to highlight blood vessels in superresolution process of retinal fundus imaging. A similar idea was explored by Liao et al. (2018) in sparse view CT reconstruction. They compute a focus map to modulate the reconstructed output to ensure that the network focused on important regions. Besides ensuring image domain data fidelity, frequency domain data fidelity is also imposed when raw K-space data is available in MR reconstruction ( Quan et al., 2018; Mardani et al., 2017; Yang et al., 2018a ).</p>
<p>基本pix2pix框架已用于低剂量CT去噪（Wolterink等人，2017b）、MR重建（Chen等人，2018b；Kim等人，2018；Dar等人，2018b；Shitrit和Raviv，2017）和PET去噪（Wang等人，2018b）。预训练VGGnet（Simonyan和Zisserman，2014）进一步纳入优化框架，以确保感知相似性（Yang等人，2018；Yu等人，2017；Yang等人，2018a；Armanious等人，2018c；Mahapatra，2017）。Yi和Babyn（2018）引入了预训练锐度检测网络，以明确限制去噪CT的锐度，尤其是低对比度区域。Mahapatra（2017）计算了局部显著性图，以突出视网膜眼底成像超分辨率过程中的血管。Liao等人（2018）在稀疏视图CT重建中探索了类似的想法。他们计算一个聚焦图来调节重建的输出，以确保网络聚焦于重要区域。除了确保图像域数据保真度外，当原始K空间数据可用于MR重建时，还可施加频域数据保真度（Quan等人，2018年；Mardani等人，2017年；Yang等人，2018a）。</p>
<p>Losses of other kinds have been used to highlight local image structures in the reconstruction, such as the saliency loss to reweight each pixel’s importance based on its perceptual relevance ( Mahapatra, 2017 ) and the style-content loss in PET denoising ( Armanious et al., 2018c ). In image reconstruction of moving organs, paired training samples are hard to obtain. Therefore, Ravì et al. (2018) proposed a physical acquisition based loss to regulate the generated image structure for endomicroscopy super resolution and Kang et al. (2019) proposed to use CycleGAN together with an identity loss in the denoising of cardiac CT. Wolterink et al. (2017b) found that in low dose CT denoising, meaningful results can still be achieved when removing the image domain fidelity loss from the pix2pix framework, but the local image structure can be altered. Papers relating to medical image reconstruction are summarized in Table 1 .</p>
<p>其他类型的损失已用于强调重建中的局部图像结构，例如基于感知相关性重新加权每个像素重要性的显著性损失（Mahapatra，2017）和PET去噪中的样式内容损失（Armanious et al.，2018c）。在运动器官的图像重建中，配对训练样本难以获得。因此，Ravì等人（2018年）提出了一种基于物理采集的损失，以调节内窥镜超分辨率生成的图像结构，Kang等人（2019年）提出在心脏CT去噪中使用CycleGAN和身份损失。 Wolterink等人（2017b）发现，在低剂量CT去噪中，当从pix2pix框架中去除图像域保真度损失时，仍然可以获得有意义的结果，但局部图像结构可以改变。表1总结了与医学图像重建相关的论文。</p>
<p>It can be noticed that the underlying methods are almost the same for all the reconstruction tasks. MR is special case as it has a well defined forward and backward operation, i.e. Fourier transform, so that raw K-space data can be incorporated. The same methodology can potentially be applied to incorporate the sinogram data in the CT reconstruction process but we have not seen any research using this idea as yet probably because the sinogram data is hard to access. The more data used, either raw K-space or image from other sequence, the better are the reconstructed results. In general, using adversarial loss produces more visually appealing results than using pixel-wise reconstruction loss alone. But using adversarial loss to match the generated and real data distribution may make the model hallucinate unseen structures. Pixel-wise reconstruction loss helps to combat this problem if paired samples are available, and if the model was trained on all healthy images but employed to reconstruct images with pathologies, the hallucination problem will still exist due to domain mismatch. Cohen et al. (2018) have conducted extensive experiments to investigate this problem and suggest that reconstructed images should not be used for direct diagnosis by radiologists unless the model has been properly verified.</p>
<p>可以注意到，所有重建任务的基本方法几乎相同。MR是一种特殊情况，因为它具有定义良好的正向和反向操作，即傅里叶变换，因此可以合并原始K空间数据。同样的方法可以潜在地应用于将正弦图数据纳入CT重建过程中，但我们尚未看到任何使用此想法的研究，可能是因为正弦图数据难以访问。使用的数据越多，无论是原始K空间还是来自其他序列的图像，重建结果越好。一般来说，使用对抗性损失比单独使用像素级重建损失产生更具视觉吸引力的结果。但是，使用对抗性损失来匹配生成的和真实的数据分布可能会使模型产生看不见的结构的幻觉。如果成对样本可用，像素级重建损失有助于解决此问题，并且如果模型在所有健康图像上训练，但用于重建具有病理学的图像，则由于域不匹配，幻觉问题仍然存在。Cohen等人（2018年）进行了大量实验来研究这一问题，并建议重建图像不应用于放射科医生的直接诊断，除非模型已得到适当验证。</p>
<p><img src="https://i.loli.net/2021/10/10/dhsHbTnY9r75DZC.png" alt="image-20211010213340155"></p>
<p>However, even though the dataset is carefully curated to match the training and testing distribution, there are other problems in further boosting performance. We have seen various different losses introduced to the pix2pix framework as shown in Table 2 to improve the reconstructed fidelity of local structures. There is, however, no reliable way of comparing their effectiveness except for relying on human observer or downstream image analysis tasks. Large scale statistical analysis by human observer is currently lacking for GAN based reconstruction methods. Furthermore, public datasets used for image reconstruction are not tailored towards further medical image analysis, which leaves a gap between upstream reconstruction and downstream analysis tasks. New reference standard datasets should be created for better comparison of these GAN-based methods.</p>
<p>然而，尽管数据集经过精心策划以匹配训练和测试分布，但在进一步提高性能方面还存在其他问题。我们已经看到pix2pix框架引入了各种不同的损失，如表2所示，以提高局部结构的重建保真度。然而，除了依靠人类观察者或下游图像分析任务外，没有可靠的方法来比较它们的有效性。目前，基于GAN的重建方法缺乏由人类观察者进行的大规模统计分析。此外，用于图像重建的公共数据集不适合进一步的医学图像分析，这在上游重建和下游分析任务之间留下了差距。应创建新的参考标准数据集，以便更好地比较这些基于GAN的方法。</p>
<p><img src="https://i.loli.net/2021/10/10/ynAJLSaCK2sD5rm.png" alt="image-20211010214402753"></p>
<h3 id="3-2-Medical-image-synthesis"><a href="#3-2-Medical-image-synthesis" class="headerlink" title="3.2 Medical image synthesis"></a>3.2 Medical image synthesis</h3><p>Depending on institutional protocols, patient consent may be required if diagnostic images are intended to be used in a publication or released into the public domain ( Clinical Practice Committee, 20 0 0 ). GANs are widely for medical image synthesis. This helps overcome the privacy issues related to diagnostic medical image data and tackle the insufficient number of positive cases of each pathology. Lack of experts annotating medical images poses another challenge for the adoption of supervised training methods.<br>Although there are ongoing collaborative efforts across multiple healthcare agencies aiming to build large open access datasets, e.g. Biobank, the National Biomedical Imaging Archive (NBIA), The Cancer Imaging Archive (TCIA) and Radiologist Society of North America (RSNA), this issue remains and constrains the number of images researchers might have access to ( Table 3 ).</p>
<p>根据机构协议，如果诊断图像拟用于出版物或发布到公共领域，则可能需要患者同意（临床实践委员会，20 0）。GANs广泛应用于医学图像合成。这有助于克服与诊断医学图像数据相关的隐私问题，并解决每个病理学的阳性病例数量不足的问题。缺乏对医学图像进行注释的专家对采用监督训练方法提出了另一个挑战。<br>尽管多个医疗机构正在进行合作，以建立大型开放存取数据集，例如Biobank、国家生物医学成像档案馆（NBIA）、癌症成像档案馆（TCIA）和北美放射科医师协会（RSNA），这个问题仍然存在，并限制了研究人员可能获得的图像数量（表3）。</p>
<p><img src="https://i.loli.net/2021/10/10/NTjGCkZ9hd2zYDI.png" alt="image-20211010214639273"></p>
<p><img src="https://i.loli.net/2021/10/10/6pTcJFyd58ln7sQ.png" alt="image-20211010214657869"></p>
<p><img src="https://i.loli.net/2021/10/10/FWp5HNtljGx8Cae.png" alt="image-20211010214715932"></p>
<p>Traditional ways to augment training sample include scaling, rotation, flipping, translation, and elastic deformation ( Simard et al., 2003 ). However, these transformations do not account for variations resulting from different imaging protocols or sequences, not to mention variations in the size, shape, location and appearance of specific pathology. GANs provide a more generic solution and have been used in numerous works for augmenting training images with promising results.</p>
<p>增强训练样本的传统方法包括缩放、旋转、翻转、平移和弹性变形（Simard et al.，2003）。然而，这些转换并没有考虑到不同成像协议或序列导致的变化，更不用说特定病理学的大小、形状、位置和外观的变化了。GANs提供了一种更通用的解决方案，并已在许多工程中用于增强训练图像，取得了良好的效果。</p>
<h4 id="3-2-1-Unconditional-synthesis"><a href="#3-2-1-Unconditional-synthesis" class="headerlink" title="3.2.1 Unconditional synthesis"></a>3.2.1 Unconditional synthesis</h4><p>Unconditional synthesis refers to image generation from random noise without any other conditional information. Techniques commonly adopted in the medical imaging community include DCGAN, WGAN, and PGGAN due to their good training stability.<br>The first two methods can handle an image resolution of up to 256 ×256 but if higher resolution images are desired, the progressive technique proposed in PGGAN is a choice. Realistic images can be generated by directly using the author released code base as long as the variations between images are not too large, for example, lung nodules and liver lesions. To make the generated images useful for downstream tasks, most studies trained a separate generator for each individual class; for example, Frid-Adar et al. (2018) used three DCGANs to generate synthetic samples for three classes of liver lesions (cysts, metastases, and hemangiomas); generated samples were found to be beneficial to the lesion classification task with both improved sensitivity and specificity when combined with real training data.<br>Bermudez et al. (2018) claimed that neuroradiologists found generated MR images to be of comparable quality to real ones, however, there were discrepancies in anatomic accuracy. Papers related to unconditional medical image synthesis are summarized in Table 4 .</p>
<p>无条件合成是指在没有任何其他条件信息的情况下，从随机噪声中生成图像。医学成像界通常采用的技术包括DCGAN、WGAN和PGGAN，因为它们具有良好的训练稳定性。<br>前两种方法可以处理高达256×256的图像分辨率，但如果需要更高分辨率的图像，则可以选择PGGAN中提出的渐进式技术。只要图像之间的差异不太大，例如肺结节和肝脏病变，就可以直接使用作者发布的代码库生成逼真的图像。为了使生成的图像对下游任务有用，大多数研究为每个单独的类训练了一个单独的生成器；例如，Frid Adar等人（2018年）使用三种DCG生成三类肝脏病变（囊肿、转移瘤和血管瘤）的合成样本；生成的样本被发现有利于病变分类任务，与真实训练数据相结合时，灵敏度和特异性都有所提高。<br>Bermudez等人（2018年）声称，神经放射科医生发现生成的MR图像质量与真实图像相当，但在解剖准确性方面存在差异。表4总结了与无条件医学图像合成相关的论文。</p>
<p><img src="https://i.loli.net/2021/10/10/vp9rPDXHjbqh67E.png" alt="image-20211010214846137"></p>
<h4 id="3-2-2-Cross-modality-synthesis"><a href="#3-2-2-Cross-modality-synthesis" class="headerlink" title="3.2.2 Cross modality synthesis"></a>3.2.2 Cross modality synthesis</h4><p>Cross modality synthesis (such as generating CT-like images based on MR images) is deemed to be useful for multiple reasons, one of which is to reduce the extra acquisition time and cost.<br>Another reason is to generate new training samples with the appearance being constrained by the anatomical structures delineated in the available modality. Most of the methods reviewed in this section share many similarities to those in Section 3.1 .<br>pix2pix-based frameworks are used in cases where different image modality data can be co-registered to ensure data fidelity.</p>
<p>跨模态合成（例如基于MR图像生成CT样图像）被认为是有用的，原因有多种，其中之一是减少额外的采集时间和成本。<br>另一个原因是生成新的训练样本，其外观受可用模式中描绘的解剖结构的约束。本节中回顾的大多数方法与第3.1节中的方法有许多相似之处。<br>基于pix2pix的框架用于不同图像模态数据可以共同配准以确保数据保真度的情况。</p>
<p>CycleGAN-based frameworks are used to handle more general cases where registration is challenging such as in cardiac applications. In a study by Wolterink et al. (2017a) for brain CT image synthesis from MR image, the authors found that training using unpaired images was even better than using aligned images. This most likely resulted from the fact that rigid registration could not very well handle local alignment in the throat, mouth, vertebrae, and nasal cavities. Hiasa et al. (2018) further incorporated gradient consistency loss in the training to improve accuracy at the boundaries. Zhang et al. (2018d) found that using only cycle loss in the cross modality synthesis was insufficient to mitigate geometric distortions in the transformation. Therefore, they employed a shape consistency loss that is obtained from two segmentors (segmentation network). Each segmentor segments the corresponding image modality into semantic labels and provides implicit shape constraints on the anatomy during translation. To make the whole system end-to-end trainable, semantic labels of training images from both modalities are required. Zhang et al. (2018c) and Chen et al. (2018a) proposed using a segmentor also in the cycle transfer using labels in only one modality. Therefore, the segmentor is trained offline and fixed during the training of the image transfer network. As reviewed in Section 2 , UNIT and CycleGAN are two equally valid frameworks for unpaired cross modality synthesis. It was found that these two frameworks performed almost equally well for the transformation between T1 and T2-weighted MR images ( Welander et al., 2018 ). Papers related to cross modality medical image synthesis are summarized in Table 5 .</p>
<p>基于CycleGAN的框架用于处理配准具有挑战性的更一般情况，例如在心脏应用中。在Wolterink et al.（2017a）关于从MR图像合成大脑CT图像的研究中，作者发现使用未配对图像进行训练甚至比使用对齐图像更好。这很可能是因为刚性配准不能很好地处理咽喉、口腔、脊椎和鼻腔的局部对齐。Hiasa等人（2018年）进一步将梯度一致性损失纳入训练中，以提高边界的准确性。Zhang等人（2018d）发现，在交叉模态合成中仅使用循环损失不足以缓解变换中的几何失真。因此，他们采用了从两个分割器（分割网络）获得的形状一致性损失。每个切割器将相应的图像模态分割成语义标签，并在翻译过程中对解剖结构提供隐式形状约束。为了使整个系统端到端可训练，需要来自两种模式的训练图像的语义标签。Zhang et al.（2018c）和Chen et al.（2018a）建议在循环传输中也使用切割器，仅在一种模式中使用标签。因此，在图像传输网络的训练期间，切割器离线训练并固定。如第2节所述，UNIT和CycleGAN是两个同样有效的非配对跨模态综合框架。研究发现，这两种框架在T1和T2加权MR图像之间的转换中表现几乎相同（Welander et al.，2018）。表5总结了与跨模态医学图像合成相关的论文。</p>
<p><img src="https://i.loli.net/2021/10/10/Y2NRZuC87qgPwet.png" alt="image-20211010215030343"></p>
<p><img src="https://i.loli.net/2021/10/10/eZnBviVLNt1WXRO.png" alt="image-20211010215105058"></p>
<h4 id="3-2-3-Other-conditional-synthesis"><a href="#3-2-3-Other-conditional-synthesis" class="headerlink" title="3.2.3 Other conditional synthesis"></a>3.2.3 Other conditional synthesis</h4><p>Medical images can be generated by constraints on segmen- tation maps, text, locations or synthetic images etc. This is useful to synthesize images in uncommon conditions, such as lung nodules touching the lung border ( Jin et al., 2018 ). Moreover, the conditioned segmentation maps can also be generated from GANs ( Guibas et al., 2017 ) or from a pretrained segmentation net- work ( Costa et al., 2017a ), by making the generation a two stage process. Mok and Chung (2018) used cGAN to augment training images for brain tumour segmentation. The generator was condi- tioned on a segmentation map and generated brain MR images in a coarse to fine manner. To ensure the tumour was well delineated with a clear boundary in the generated image, they further forced the generator to output the tumour boundaries in the generation process. The full list of synthesis works is summarized in Table 6 .</p>
<p>医学图像可以通过对分割图、文本、位置或合成图像等的约束生成。这对于在罕见情况下合成图像非常有用，例如肺结节触及肺边界（Jin等人，2018）。此外，通过将生成过程分为两个阶段，也可以从GANs（Guibas等人，2017）或预训练分割网络（Costa等人，2017a）生成条件分割图。Mok和Chung（2018）使用cGAN增强用于脑肿瘤分割的训练图像。该发生器在分割图上进行调节，并以从粗到精的方式生成大脑MR图像。为了确保生成的图像中肿瘤边界清晰，他们进一步强制生成器在生成过程中输出肿瘤边界。表6总结了综合工作的完整清单。</p>
<p><img src="https://i.loli.net/2021/10/10/vTxdQpm7NagLrE1.png" alt="image-20211010220240451"></p>
<h3 id="3-3-Segmentation"><a href="#3-3-Segmentation" class="headerlink" title="3.3 Segmentation"></a>3.3 Segmentation</h3><p>Generally, researchers have used pixel-wise or voxel-wise loss such as cross entropy for segmentation. Despite the fact that U-net ( Ronneberger et al., 2015 ) was used to combine both low-level and high-level features, there is no guarantee of spatial consistency in the final segmentation map. Traditionally, conditional random field (CRF) and graph cut methods are usually adopted for segmentation refinement by incorporating spatial correlation. Their limitation is that they only take into account pair-wise potentials which might cause serious boundary leakage in low contrast regions. On the other hand, adversarial losses as introduced by the discriminator can take into account high order potentials ( Yang et al., 2017a ). In this case, the discriminator can be regarded as a shape regulator. This regularization effect is more prominent when the object of interest has a compact shape, e.g. for lung and heart mask but less useful for deformable objects such as vessels and catheters. This regulation effect can be also applied to the internal features of the segmentor to achieve domain (different scanners, imaging protocols, modality) invariance ( Kamnitsas et al., 2017; Dou et al., 2018 ). The adversarial loss can also be viewed as a adaptively learned similarity measure between the segmented outputs and the annotated groundtruth. Therefore, instead of measuring the similarity in the pixel domain, the discriminative network projects the input to a low dimension manifold and measures the similarity there. The idea is similar to the perceptual loss. The difference is that the perceptual loss is computed from a pre-trained classification network on natural images whereas the adversarial loss is computed from a network that trained adaptively during the evolvement of the generator.</p>
<p>通常，研究人员使用像素级或体素级的损失，如交叉熵进行分割。尽管U-net（Ronneberger et al.，2015）被用于结合低层和高层特征，但无法保证最终分割图的空间一致性。传统上，通常采用条件随机场（CRF）和图切割方法，通过结合空间相关性进行分割细化。它们的局限性在于，它们只考虑了在低对比度区域可能导致严重边界泄漏的成对电势。另一方面，鉴别器引入的对抗性损失可以考虑高阶电位（Yang等人，2017a）。在这种情况下，鉴别器可被视为形状调节器。当感兴趣的对象具有紧凑的形状时，这种正则化效果更为显著，例如用于肺和心脏面罩，但对于可变形对象（如血管和导管）不太有用。该调节效应也可应用于分节器的内部特征，以实现域（不同扫描仪、成像协议、模态）不变性（Kamnitsas等人，2017年；Dou等人，2018年）。对抗性损失也可以被视为分段输出和注释背景真相之间的自适应学习相似性度量。因此，区别网络不是在像素域中测量相似性，而是将输入投影到低维流形并在那里测量相似性。这种想法类似于知觉丧失。不同之处在于，感知损失是通过对自然图像进行预训练的分类网络计算的，而对抗损失是通过在生成器演化过程中进行自适应训练的网络计算的。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h4 id="最大对数似然目标"><a href="#最大对数似然目标" class="headerlink" title="最大对数似然目标"></a>最大对数似然目标</h4><h4 id="鞍点优化"><a href="#鞍点优化" class="headerlink" title="鞍点优化"></a>鞍点优化</h4><h4 id="JS散度"><a href="#JS散度" class="headerlink" title="JS散度"></a>JS散度</h4><h4 id="模式崩溃"><a href="#模式崩溃" class="headerlink" title="模式崩溃"></a>模式崩溃</h4><h4 id="f散度"><a href="#f散度" class="headerlink" title="f散度"></a>f散度</h4><h4 id="铰链损失"><a href="#铰链损失" class="headerlink" title="铰链损失"></a>铰链损失</h4><h4 id="Wasserstein距离"><a href="#Wasserstein距离" class="headerlink" title="Wasserstein距离"></a>Wasserstein距离</h4><h4 id="自动编码器架构"><a href="#自动编码器架构" class="headerlink" title="自动编码器架构"></a>自动编码器架构</h4><h4 id="完全卷积采样"><a href="#完全卷积采样" class="headerlink" title="完全卷积采样"></a>完全卷积采样</h4><h4 id="分割掩模"><a href="#分割掩模" class="headerlink" title="分割掩模"></a>分割掩模</h4></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Farewellswind</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://lethewind.github.io/2021/10/11/Generative%20adversarial%20network%20in%20medical%20imaging%20A%20review/">http://lethewind.github.io/2021/10/11/Generative%20adversarial%20network%20in%20medical%20imaging%20A%20review/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://lethewind.github.io" target="_blank">Lethe</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GAN-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C-Medical-imaging-%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F/">GAN, 生成对抗网络, Medical imaging, 医学图像</a></div><div class="post_share"><div class="social-share" data-image="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/"><img class="next-cover" src="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习相关名词解释</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://z3.ax1x.com/2021/09/18/4lguXd.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Farewellswind</div><div class="author-info__description">Blog to record sth. powered by GitPages & Hexo</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Lethewind"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Lethewind" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:farewell.summer.wind@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">没钱租服务器了，就用GitPages吧</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Generative-adversarial-network-in-medical-imaging-A-review"><span class="toc-text">Generative adversarial network in medical imaging: A review</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Background"><span class="toc-text">2. Background</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Vanilla-GAN"><span class="toc-text">2.1 Vanilla GAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Challenges-in-optimizing-GANs"><span class="toc-text">2.2 Challenges in optimizing GANs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Variants-of-GANs"><span class="toc-text">2.3 Variants of GANs</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-Varying-objective-of-D"><span class="toc-text">2.3.1 Varying objective of D</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-Varying-object-of-G"><span class="toc-text">2.3.2 Varying object of G</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-Varying-architecture"><span class="toc-text">2.3.3 Varying architecture</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Applications-in-medical-imaging"><span class="toc-text">3. Applications in medical imaging</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Reconstruction"><span class="toc-text">3.1 Reconstruction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Medical-image-synthesis"><span class="toc-text">3.2 Medical image synthesis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-Unconditional-synthesis"><span class="toc-text">3.2.1 Unconditional synthesis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-Cross-modality-synthesis"><span class="toc-text">3.2.2 Cross modality synthesis</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-Other-conditional-synthesis"><span class="toc-text">3.2.3 Other conditional synthesis</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Segmentation"><span class="toc-text">3.3 Segmentation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E7%9B%AE%E6%A0%87"><span class="toc-text">最大对数似然目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9E%8D%E7%82%B9%E4%BC%98%E5%8C%96"><span class="toc-text">鞍点优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#JS%E6%95%A3%E5%BA%A6"><span class="toc-text">JS散度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%BC%8F%E5%B4%A9%E6%BA%83"><span class="toc-text">模式崩溃</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#f%E6%95%A3%E5%BA%A6"><span class="toc-text">f散度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%93%B0%E9%93%BE%E6%8D%9F%E5%A4%B1"><span class="toc-text">铰链损失</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Wasserstein%E8%B7%9D%E7%A6%BB"><span class="toc-text">Wasserstein距离</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84"><span class="toc-text">自动编码器架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E5%85%A8%E5%8D%B7%E7%A7%AF%E9%87%87%E6%A0%B7"><span class="toc-text">完全卷积采样</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%89%B2%E6%8E%A9%E6%A8%A1"><span class="toc-text">分割掩模</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/10/11/Generative%20adversarial%20network%20in%20medical%20imaging%20A%20review/" title="Generative adversarial network in medical imaging A review"><img src="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Generative adversarial network in medical imaging A review"/></a><div class="content"><a class="title" href="/2021/10/11/Generative%20adversarial%20network%20in%20medical%20imaging%20A%20review/" title="Generative adversarial network in medical imaging A review">Generative adversarial network in medical imaging A review</a><time datetime="2021-10-11T05:41:49.000Z" title="发表于 2021-10-11 13:41:49">2021-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/" title="机器学习相关名词解释"><img src="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习相关名词解释"/></a><div class="content"><a class="title" href="/2021/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A/" title="机器学习相关名词解释">机器学习相关名词解释</a><time datetime="2021-10-11T05:39:23.000Z" title="发表于 2021-10-11 13:39:23">2021-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/10/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9F%BA%E4%BA%8E%E6%B5%99%E5%A4%A7MOOC%EF%BC%89/" title="数据结构笔记（基于浙大MOOC）"><img src="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构笔记（基于浙大MOOC）"/></a><div class="content"><a class="title" href="/2021/10/11/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9F%BA%E4%BA%8E%E6%B5%99%E5%A4%A7MOOC%EF%BC%89/" title="数据结构笔记（基于浙大MOOC）">数据结构笔记（基于浙大MOOC）</a><time datetime="2021-10-11T05:30:24.000Z" title="发表于 2021-10-11 13:30:24">2021-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" title="激活函数"><img src="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="激活函数"/></a><div class="content"><a class="title" href="/2021/09/18/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" title="激活函数">激活函数</a><time datetime="2021-09-18T04:17:19.000Z" title="发表于 2021-09-18 12:17:19">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/hello-world/" title="Hello World"><img src="https://z3.ax1x.com/2021/09/18/4lfS4x.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Hello World"/></a><div class="content"><a class="title" href="/2021/09/18/hello-world/" title="Hello World">Hello World</a><time datetime="2021-09-18T03:18:50.091Z" title="发表于 2021-09-18 11:18:50">2021-09-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Farewellswind</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '56NtfYiSI7GJRvsLOlYS7NPO-gzGzoHsz',
      appKey: 'uLXVuEEcObBK9s9LXVvEnN1K',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>